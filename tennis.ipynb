{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradients (MADDPG)\n",
    "---\n",
    "In this notebook, I train Unity's Tennis environment using a multi-agent DDPG algorithm.\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128        # minibatch size\n",
    "BUFFER_SIZE = 500000  # replay buffer size\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "TAU = 0.06              # for soft update of target parameters\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "UPDATE_EVERY = 2        # time steps between network updates\n",
    "N_UPDATES = 1           # number of times training\n",
    "ADD_NOISE = True\n",
    "\n",
    "#epsilon greedy params for exploration vs exploatation\n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for better initial weight setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size*2, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size*2, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+(action_size*2), fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class to store experiences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random noise class\n",
    "Ornstein Uhlenbeck process helps better than random noise in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0.0, theta=0.13, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent definition\n",
    "Handles co-joint training of both agents handling the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            num_agents (int): number of agents\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.eps = EPS_START\n",
    "        self.step_no = 0\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "                \n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, agent_number):\n",
    "        self.step_no += 1\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "        # Learn, if enough samples are available in memory and at interval settings\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            if self.step_no % UPDATE_EVERY == 0:\n",
    "                for i in range(N_UPDATES):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, GAMMA, agent_number)\n",
    "\n",
    "    def act(self, states, add_noise = True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.num_agents, self.action_size))\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = self.actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "        self.actor_local.train()\n",
    "        #if add_noise:\n",
    "        #    actions += self.eps * self.noise.sample()\n",
    "        if add_noise and (np.random.random() < self.eps):\n",
    "            actions += self.noise.sample()\n",
    "            #update the exploration parameter\n",
    "            self.eps -= EPS_DECAY\n",
    "            if self.eps < EPS_END:\n",
    "                self.eps = EPS_END\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma, agent_number):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        \n",
    "        if agent_number == 0:\n",
    "            actions_next = torch.cat((actions_next, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_next = torch.cat((actions[:,:2], actions_next), dim=1)\n",
    "            \n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # differntiate between the two, slice the initial action space\n",
    "        if agent_number == 0:\n",
    "            actions_pred = torch.cat((actions_pred, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_pred = torch.cat((actions[:,:2], actions_pred), dim=1)\n",
    "\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "        # Update epsilon noise value\n",
    "        #self.eps = self.eps - EPS_DECAY\n",
    "        #if self.eps < EPS_END:\n",
    "        #    self.eps=EPS_END\n",
    "                  \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(source_param.data)            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Tennis_Linux_NoVis/Tennis.x86_64', seed=1)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "#print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_one = Agent(state_size, action_size, 1, random_seed=0)\n",
    "agent_two = Agent(state_size, action_size, 1, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tMax Reward: 0.09\tAverage Reward: 0.090\n",
      "Episode 50\tMax Reward: 0.09\tAverage Reward: 0.007\n",
      "Episode 100\tMax Reward: 0.00\tAverage Reward: 0.012\n",
      "Episode 150\tMax Reward: 0.00\tAverage Reward: 0.014\n",
      "Episode 200\tMax Reward: 0.00\tAverage Reward: 0.018\n",
      "Episode 250\tMax Reward: 0.00\tAverage Reward: 0.028\n",
      "Episode 300\tMax Reward: 0.09\tAverage Reward: 0.027\n",
      "Episode 350\tMax Reward: 0.00\tAverage Reward: 0.031\n",
      "Episode 400\tMax Reward: 0.00\tAverage Reward: 0.054\n",
      "Episode 450\tMax Reward: 0.30\tAverage Reward: 0.088\n",
      "Episode 500\tMax Reward: 0.10\tAverage Reward: 0.117\n",
      "Episode 550\tMax Reward: 0.09\tAverage Reward: 0.138\n",
      "Episode 600\tMax Reward: 0.10\tAverage Reward: 0.154\n",
      "Episode 650\tMax Reward: 0.10\tAverage Reward: 0.203\n",
      "Episode 700\tMax Reward: 0.10\tAverage Reward: 0.252\n",
      "Episode 750\tMax Reward: 1.00\tAverage Reward: 0.299\n",
      "Episode 800\tMax Reward: 0.40\tAverage Reward: 0.462\n",
      "\n",
      "Environment solved in 829 episodes!\tAverage Score: 0.501\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_all = []\n",
    "averages = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations\n",
    "    states = np.reshape(states, (1,48))\n",
    "    agent_one.reset()\n",
    "    agent_two.reset()\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        action_one = agent_one.act(states)           # agent 1 chooses an action\n",
    "        action_two = agent_two.act(states)           # agent 2 chooses an action\n",
    "        # concatenate the actions and reshape them for the environment\n",
    "        actions = np.reshape(np.concatenate((action_one, action_two), axis=0),(1,4)) \n",
    "        # send both agents' actions together to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        # get next states and reshape them \n",
    "        next_states = np.reshape(env_info.vector_observations,(1,48))         \n",
    "        rewards = env_info.rewards                         # get reward\n",
    "        done = env_info.local_done                         # see if episode finished\n",
    "        agent_one.step(states, actions, rewards[0], next_states, done, 0) # train agent 1\n",
    "        agent_two.step(states, actions, rewards[1], next_states, done, 1) # train agent 2\n",
    "        \n",
    "        \n",
    "        scores += rewards                                  # update the score for each agent\n",
    "        states = next_states                               # roll over states to next time step\n",
    "\n",
    "        if np.any(done):                                  \n",
    "            break\n",
    "\n",
    "    max_score = np.max(scores)\n",
    "    scores_window.append(max_score)\n",
    "    scores_all.append(max_score)\n",
    "    averages.append(np.mean(scores_window))\n",
    "    # print training progression every now and then            \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {}\\tMax Reward: {:.2f}\\tAverage Reward: {:.3f}'.format(\n",
    "            episode, max_score, np.mean(scores_window)))\n",
    "    # check if we have reached the solved condition\n",
    "    # if yes, print results and save agent networks weights for later use\n",
    "    # and exit the training cycle\n",
    "    if np.mean(scores_window)>=0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(\n",
    "            episode, np.mean(scores_window)))\n",
    "        torch.save(agent_one.actor_local.state_dict(), 'checkpoint_actor_one.pth')\n",
    "        torch.save(agent_one.critic_local.state_dict(), 'checkpoint_critic_one.pth')\n",
    "        torch.save(agent_two.actor_local.state_dict(), 'checkpoint_actor_two.pth')\n",
    "        torch.save(agent_two.critic_local.state_dict(), 'checkpoint_critic_two.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_all)+1), scores_all)\n",
    "plt.plot(np.arange(1, len(averages)+1), averages)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispose the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
